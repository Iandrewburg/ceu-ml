---
title: "CEU Machine Learning Concepts - Lab 1"
author: János Divényi
output: html_notebook
---


```{r}
library(tidyverse)
library(glmnet)
theme_set(theme_minimal())
```

## Our problem

Let's start with a simple linear model:

$$
Y = X'\beta + \varepsilon = \beta_1 X_1 + \beta_2 X_2 + ... + \beta_p X_p + \varepsilon
$$

This model is _approximately sparse_, meaning that most of the coefficients are close to zero:

```{r}
generateBeta <- function(beta_length) {
    4 / seq(beta_length)^2
}
qplot(seq(20), generateBeta(20), geom = "col", xlab = "x[i]", ylab = "beta")
```


```{r}
f_y_x <- function(x) {
    beta <- generateBeta(dim(x)[2])  # approximately sparse model
    x %*% beta
}
```




## Recap: Prediction

We would like to construct a predictor based on $X$:
$\hat Y = f(X)$.

The optimal predictor (in terms of squared loss) is the conditional expectation function:
$f^*(X) = \text{E}[Y|X] = X'\beta$.

Let's assume that we know that the conditional expectation function is linear. Still, we need to know $\beta$ as well to have the optimal predictor. As we do not know the parameters we need to estimate them and use the estimated predictor function $\hat f(X)$ instead.

We would like to be as close to the optimal predictor as possible. So we would like to minimize the error: $f^*(X) - \hat f(X)$.

Our main goal is to predict Y for a new observation $X = x_0$, so we would like to minimize the expected squared loss $\text{E} \left[\left(f^*(x_0) - \hat f(x_0)\right)^2\right]$. This could be decomposed into two terms, expressing the famous bias-variance trade-off:

$$
\text{E} \left[\left(f^*(x_0) - \hat f(x_0)\right)^2\right] =\ ...\ = \text{E}^2\left[f^*(x_0) - \hat f(x_0)\right] + \text{Var}\left(\hat f(x_0)\right)
$$



## Penalized linear regression: LASSO

```{r}
n <- 200
p <- 100
x <- matrix(rnorm(n * p), nrow = n, ncol = p)	

y_exp <- f_y_x(x)
y <- y_exp + rnorm(n) * 4
```

```{r}
calculateMSE <- function(prediction, y_exp) {
    mean((prediction - y_exp)^2)
}
```

```{r}
# Try some models and evaluate their overall performance
```



```{r}
# Be systematic about choosing the best penalty parameter
lambda_values <- seq(0, 1, 0.05)
```

## Simulation 

```{r}
# Write a simulator function that can evaluate a model for a given X
# end result should be a data.frame with columns of lambda, prediction (f_star), error
run_simulation <- function(x_generator, x0 = 0.1, lambdas = seq(0, 0.6, 0.02)) {
    # generate the sample
    # x <-  
    # y_exp <-
    # y <- 
    
    # generate the x0 value
    # x_eval <- matrix(x0, ncol = dim(x)[2])
    
    # run the simulation for each lambda and store the results
    
}
```

```{r}
visualize_simulation_results <- function(simulation_results) {
    group_by(simulation_results, lambda) |>
    summarise(bias2 = mean(error)^2, var = var(f_star)) |>
    mutate(MSE = bias2 + var) |>
    pivot_longer(bias2:MSE, names_to = "metric") |>
    mutate(metric = factor(metric, levels = c("bias2", "var", "MSE"))) |>
    ggplot(aes(lambda, value, color = metric)) + geom_line(size = 1)    
}
```


### Scenario #1: Independent predictors

```{r}
generate_independent_predictors <- function(n = 200, p = 100) {
    matrix(rnorm(n * p), nrow = n, ncol = p)
}
```


```{r}
# run simulation for n_sim times
# simulation_results <- 
```


```{r}
visualize_simulation_results(simulation_results)
```


### Simulation scenario #2: Correlated predictors

The above model might seem unrealistic in most cases: you rarely have 100 independent predictors linearly affecting your outcome variable. However, you can use the same method to estimate more flexible models as well, e.g. by allowing for squares and interactions: with just 13 predictors you can end up with as much as 104 features which is very close to our previous model (only that now our features are correlated).

```{r}
generate_correlated_predictors <- function(n = 200, p = 13) {
    # for p = 13, there will be 104 features
    poly(matrix(rnorm(n * p), nrow = n, ncol = p), degree = 2, raw = TRUE)
}
```

```{r}
# run simulation for n_sim times
# simulation_results <- 
```


```{r}
visualize_simulation_results(simulation_results)
```