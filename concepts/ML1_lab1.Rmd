---
title: "CEU Machine Learning Concepts - Lab 1"
author: "János K. Divényi"
output:
  html_document:
    df_print: paged
---


```{r setup, message=FALSE}
library(tidyverse)
library(skimr)

options(scipen = 99)  # avoid scientific notation
options(max.print = 20)  # avoid long outputs
options(str = strOptions(list.len = 20))  # avoid long outputs
theme_set(theme_minimal())
```

## Our problem

Let's start with a simple linear model:

$$
Y = X'\beta + \varepsilon = \beta_1 X_1^2 + \beta_2 X_1 + \varepsilon
$$
We have 2 variables available $X = (X_1, X_2)$ but only one of them is related to the outcome.

The true model is:

$$
Y = X_1^2 - 1.5  X_1+ \varepsilon
$$

## Estimation

```{r model}
f_y_x <- function(x1) {
    x1^2 - 1.5 * x1
}
```

```{r generate-data}
n <- 100
set.seed(20230206)

data <- tibble(
    x1 = runif(n),
    x2 = runif(n),
    y = f_y_x(x1) + rnorm(n)
)
```

```{r know-your-data}
skim(data)
GGally::ggpairs(data)
```


```{r estimation}
f_hat_x <- lm(y ~ x1 + I(x1^2), data)
f_hat_x
```

```{r evaluation}
point_to_evaluate <- list(x1 = 0, x2 = 0)
list(
    true_value = f_y_x(point_to_evaluate$x1),
    predicted_value = as.numeric(predict(f_hat_x, newdata = point_to_evaluate))
)
```

```{r estimate-simpler-model}
f_hat_simple_x <- lm(y ~ x1, data)
f_hat_simple_x

list(
    true_value = f_y_x(point_to_evaluate$x1),
    predicted_value = as.numeric(predict(f_hat_x, newdata = point_to_evaluate)),
    predicted_value_simple = as.numeric(predict(f_hat_simple_x, newdata = point_to_evaluate))
)
```


## The magical `map`

The map functions in the `purrr` package (part of `tidyverse`) _"transform their input by applying a function to each element of a list or atomic vector and returning an object of the same length as the input"_. ([Source](https://purrr.tidyverse.org/reference/map.html))

```{r calculate-sample-average}
mean(rnorm(1000))
```

```{r calculate-sample-average-for-different-sizes}
map(c(5, 10, 20), rnorm)
```


```{r calculate-sample-average-mc}
n <- 100
n_sim <- 10000
# use map to generate sample averages of 100 standard normal variables - 10k times
# plot the sampling distribution
```

```{r nested-maps}
sds <- c(1, 5)
# repeat the previous exercise but know for standard normal AND normal with sd=5 variables
# nest maps:
# 1. apply the random number generation and avg calculation to each simulation step
# 2. apply the simulation to each possible value of sigma

# plot the results
```

## Simulation

```{r simulation-function}
runSimulationStep <- function(n = 100, sd_e = 1) {
    # Step 1: Simulate the data (points 1-3 of the lecture)
    
    # Step 2: Estimate models (point 4 of the lecture - without evaluating on a given point)
}
```

```{r sample-simulation}
estimated_models <- runSimulationStep()
estimated_models
```

```{r evaluate-simulation-step}
# (remaining part of point 4 of the lecture)
point_to_evaluate <- list(x1 = 0, x2 = 0)
map(estimated_models, ~as.numeric(predict(.x, newdata = point_to_evaluate)))
```

```{r run-mc-simulation}
# run the simulation for multiple times (point 5 of the lecture)
# do not forget to set the seed

```


```{r check-result}
str(simulated_models, max.level = 1)
simulated_models[[1]]
```

```{r evaluate-at-given-point}
# Evaluate the models in the (0, 0) point
# (use a nested map: map over all the simulation runs and evaluate all the estimated models)
predictions00 <- 
```


```{r evaluate-simulation-results-chart}
pivot_longer(predictions00, cols = everything()) |>
    ggplot(aes(value, color = name, fill = name)) + geom_density(alpha = 0.5)
```


```{r evaluate-simulation-results-table}
# Calculate the metrics (point 6 of the lecture)

```

## Get some intuition for bias-variance trade-off

```{r evaluate-for-multiple-points}
points_to_evaluate <- tibble(x1 = seq(0, 1, 0.05), x2 = 0)
predict(f_hat_x, newdata = points_to_evaluate)
map(simulated_models[[1]], ~predict(.x, newdata = points_to_evaluate)) |> bind_cols()
```


```{r multiple-points-evaluation-with-chart}
map_df(seq_along(simulated_models[1:100]), ~{
    i <- .x
    map(simulated_models[[i]], ~predict(.x, newdata = points_to_evaluate)) |> 
        bind_cols() |>
        mutate(simulation_run = i, x1 = points_to_evaluate$x1)
}) |>
    pivot_longer(cols = true:full, names_to = "model", values_to = "prediction") |>
    ggplot(aes(x1, prediction, group = simulation_run, col = model)) +
        geom_line(alpha = 0.3) +
        facet_wrap(~ model)
```

[Bias-variance Shiny App](https://divenyijanos.shinyapps.io/bias-variance-app/)