---
title: "CEU Machine Learning Concepts - Lab 1"
author: János Divényi
output: html_notebook
---


```{r message=FALSE}
library(tidyverse)
library(glmnet)
theme_set(theme_minimal())
```

## Our problem

Let's start with a simple linear model:

$$
Y = X'\beta + \varepsilon = \beta_1 X_1 + \beta_2 X_2 + ... + \beta_p X_p + \varepsilon
$$

This model is _approximately sparse_, meaning that most of the coefficients are close to zero:

```{r}
generateBeta <- function(beta_length) {
    4 / seq(beta_length)^2
}
qplot(seq(20), generateBeta(20), geom = "col", xlab = "x[i]", ylab = "beta")
```


```{r}
f_y_x <- function(x) {
    beta <- generateBeta(dim(x)[2])  # approximately sparse model
    x %*% beta
}
```




## Recap: Prediction

We would like to construct a predictor based on $X$:
$\hat Y = f(X)$.

The optimal predictor (in terms of squared loss) is the conditional expectation function:
$f^*(X) = \text{E}[Y|X] = X'\beta$.

Let's assume that we know that the conditional expectation function is linear. Still, we need to know $\beta$ as well to have the optimal predictor. As we do not know the parameters we need to estimate them and use the estimated predictor function $\hat f(X)$ instead.

We would like to be as close to the optimal predictor as possible. So we would like to minimize the error: $f^*(X) - \hat f(X)$.

Our main goal is to predict Y for a new observation $X = x_0$, so we would like to minimize the expected squared loss $\text{E} \left[\left(f^*(x_0) - \hat f(x_0)\right)^2\right]$. This could be decomposed into two terms, expressing the famous bias-variance trade-off:

$$
\text{E} \left[\left(f^*(x_0) - \hat f(x_0)\right)^2\right] =\ ...\ = \text{E}^2\left[f^*(x_0) - \hat f(x_0)\right] + \text{Var}\left(\hat f(x_0)\right)
$$



## Penalized linear regression: LASSO

```{r}
n <- 200
p <- 100
set.seed(20220216)
x <- matrix(rnorm(n * p), nrow = n, ncol = p)	

y_exp <- f_y_x(x)
y <- y_exp + rnorm(n) * 4
```

```{r}
calculateMSE <- function(prediction, y_exp) {
    mean((prediction - y_exp)^2)
}
```

```{r}
# Try some models and evaluate their overall performance
simple_lm <- lm(y ~ x)
calculateMSE(predict(simple_lm), y_exp) # note that we compare to E[Y|X]

simple_linreg <- glmnet(x, y, alpha = 1, lambda = 0) # same as lm
calculateMSE(predict(simple_linreg, newx = x), y_exp)

simple_lasso <- glmnet(x, y, alpha = 1, lambda = 0.1)
calculateMSE(predict(simple_lasso, newx = x), y_exp)

simple_lasso <- glmnet(x, y, alpha = 1, lambda = 0.5)
calculateMSE(predict(simple_lasso, newx = x), y_exp)
```



```{r}
# Be systematic about choosing the best penalty parameter
lambda_values <- seq(0, 1, 0.05)
results <- map_df(lambda_values, ~{
    model <- glmnet(x, y, alpha = 1, lambda = .x)
    mse <- calculateMSE(predict(model, newx = x), y_exp)    
    tibble(lambda = .x, MSE = mse)
})
ggplot(results, aes(lambda, MSE)) + geom_line(size = 1)
```
Even if we know that the true model is linear we are better off (in terms of mean squared loss) not solving the original problem but a "distorted" one where we introduce penalty for many parameters. The reason for this is that we have not enough information to uncover all of the true parameter values. We can improve our prediction accuracy focusing only on the parameters that matter more.

This would not be true if we had much more information (or, equivalently, less variables in the model).

```{r}
n <- 200000
x <- matrix(rnorm(n * p), nrow = n, ncol = p)	
y_exp <- f_y_x(x)
y <- y_exp + rnorm(n) * 4

results_large_n <- map_df(seq(0, 0.3, 0.05), ~{
    model <- glmnet(x, y, alpha = 1, lambda = .x)
    mse <- calculateMSE(predict(model, newx = x), y_exp)    
    tibble(lambda = .x, MSE = mse)
})
ggplot(results_large_n, aes(lambda, MSE)) + geom_line(size = 1)
```

Here, we can estimate all of the parameters quite well so we are close to zero loss with the regular regression model. Deliberately distorting our optimization problem just harms.