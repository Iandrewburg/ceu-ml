---
title: "CEU Machine Learning Concepts - Lab 1"
author: "János K. Divényi"
output:
  html_document:
    df_print: paged
---


```{r setup, message=FALSE}
library(tidyverse)
library(skimr)

options(scipen = 99)  # avoid scientific notation
options(max.print = 20)  # avoid long outputs
options(str = strOptions(list.len = 20))  # avoid long outputs
theme_set(theme_minimal())
```

## Our problem

Let's start with a simple linear model:

$$
Y = X'\beta + \varepsilon = \beta_1 X_1^2 + \beta_2 X_1 + \varepsilon
$$
We have 2 variables available $X = (X_1, X_2)$ but only one of them is related to the outcome.

The true model is:

$$
Y = X_1^2 - 1.5  X_1+ \varepsilon
$$

## Estimation

```{r model}
f_y_x <- function(x1) {
    x1^2 - 1.5 * x1
}
```

```{r generate-data}
n <- 100
set.seed(20230206)

data <- tibble(
    x1 = runif(n),
    x2 = runif(n),
    y = f_y_x(x1) + rnorm(n)
)
skim(data)
GGally::ggpairs(data)
```


```{r estimation}
f_hat_x <- lm(y ~ x1 + I(x1^2), data)
f_hat_x
```

```{r evaluation}
point_to_evaluate <- list(x1 = 0, x2 = 0)
list(
    true_value = f_y_x(point_to_evaluate$x1),
    predicted_value = as.numeric(predict(f_hat_x, newdata = point_to_evaluate))
)
```

```{r estimate-simpler-model}
f_hat_simple_x <- lm(y ~ x1, data)
f_hat_simple_x

list(
    true_value = f_y_x(point_to_evaluate$x1),
    predicted_value = as.numeric(predict(f_hat_x, newdata = point_to_evaluate)),
    predicted_value_simple = as.numeric(predict(f_hat_simple_x, newdata = point_to_evaluate))
)
```


## The magical `map`

The map functions in the `purrr` package (part of `tidyverse`) _"transform their input by applying a function to each element of a list or atomic vector and returning an object of the same length as the input"_. ([Source](https://purrr.tidyverse.org/reference/map.html))

```{r calculate-sample-average}
mean(rnorm(1000))
```

```{r calculate-sample-average-for-different-sizes}
map(c(5, 10, 20), rnorm)
```


```{r calculate-sample-average-mc}
n <- 100
n_sim <- 10000
map_dbl(
    seq(n_sim),
    ~mean(rnorm(n))
) |> 
    hist(
        main = glue::glue("Sampling distribution of the mean of {n} standard normal variables"),
        xlab = ""
    )
```

```{r nest-maps}
sds <- c(1, 5)
map(sds, function(s) {
    map_dbl(
        seq(n_sim),
        ~mean(rnorm(n, sd = s))
    )
})
```

```{r plot-nested-maps}
map_df(sds, function(s) {
    tibble(
        sigma = as.factor(s),
        sample_avg = map_dbl(
            seq(n_sim),
            ~mean(rnorm(n, sd = s))
        )
    )
}) |> ggplot(aes(sample_avg, fill = sigma, color = sigma)) + 
    geom_density(alpha = 0.5) +
    labs(
        title = glue::glue("Sampling distribution of the mean of {n} normal variables"),
        x = ""
    )
```

## Simulation

```{r simulation-function}
runSimulationStep <- function(n = 100, sd_e = 1) {
    # Step 1: Simulate the data
    data <- tibble(
        x1 = runif(n),
        x2 = runif(n),
        y = f_y_x(x1) + rnorm(n, sd = sd_e)
    )
    
    # Step 2: Estimation
    list(
        true = lm(y ~ x1 + I(x1^2), data),
        simple = lm(y ~ x1, data),
        full = lm(y ~ polym(x1, x2, degree = 2, raw = TRUE), data)
    )
}
```

```{r sample-simulation}
estimated_models <- runSimulationStep()
estimated_models
```

```{r evaluate-simulation-step}
point_to_evaluate <- list(x1 = 0, x2 = 0)
map(estimated_models, ~as.numeric(predict(.x, newdata = point_to_evaluate)))
```

```{r run-mc-simulation}
set.seed(20230206)

n_sim <- 1000
simulated_models <- map(seq(n_sim), ~runSimulationStep())
```


```{r check-result}
str(simulated_models, max.level = 1)
simulated_models[[1]]
```

```{r evaluate-at-given-point}
predictions00 <- map_df(simulated_models, ~{
    map(.x, ~as.numeric(predict(.x, newdata = point_to_evaluate))) |> as_tibble()
})
```


```{r evaluate-simulation-results-chart}
pivot_longer(predictions00, cols = everything()) |>
    ggplot(aes(value, color = name, fill = name)) + geom_density(alpha = 0.5)
```


```{r evaluate-simulation-results-table}
pivot_longer(predictions00, cols = everything()) |>
    group_by(name) |>
    summarise(
        bias = mean(value) - f_y_x(point_to_evaluate$x1),
        var = var(value),
        MSE = mean((value - f_y_x(point_to_evaluate$x1))^2)
    )
```

Note that on average the simpler model gave better predictions than the true model.
Tip: simulate with `n=500` or `sd_e = 0.1` and compare those results with these.
Can you explain the difference?

## Get some intuition for bias-variance trade-off

To get some intuition as to why the simpler (wrong) model performed better, let's take a look at the predictions for different values of $X_1$. Here we take advantage of the fact that the `predict()` function also works with multiple values of the features.

```{r evaluate-for-multiple-points}
points_to_evaluate <- tibble(x1 = seq(0, 1, 0.05), x2 = 0)
predict(f_hat_x, newdata = points_to_evaluate)
```

```{r evaluate-simulation-results-for-multiple-points}
predictions <- map_df(seq_along(simulated_models[1:50]), function(i) {
    map(simulated_models[[i]], ~predict(.x, newdata = points_to_evaluate)) |> 
        bind_cols() |>
        mutate(simulation_run = i, x1 = points_to_evaluate$x1)
})
```

```{r multiple-points-evaluation-chart}
predictions |>
    mutate(true_value = f_y_x(x1)) |>
    pivot_longer(cols = true:full, names_to = "model", values_to = "prediction") |>
    ggplot(aes(x1, prediction, group = simulation_run, col = model)) +
        geom_line(alpha = 0.3) +
        geom_line(aes(y = true_value), color = "black", linetype = "dashed") +
        facet_wrap(~ model)
```
The true relationship is indicated by the dashed line. It is clear that the simple model has no chance of revealing the true relationship, but it is still closer to it on average, since the true model (not to mention the full model) is highly variable.

We could also repeat the previous exercise with `n=1000` as well. More observations give more information so it might be possible to estimate the complex models more accurately. You should be able to see the difference in the following plot.

```{r evaluate-simulation-results-for-multiple-points-with-different-nobs}
nobs <- c(100, 10000)
map_df(nobs, function(n) {
    simulated_models <- map(seq(10), ~runSimulationStep(n = n))
    map_df(seq_along(simulated_models), function(i) {
        map(simulated_models[[i]], ~predict(.x, newdata = points_to_evaluate)) |> 
            bind_cols() |>
            mutate(n = n, simulation_run = i, x1 = points_to_evaluate$x1)
    })
}) |>
    mutate(true_value = f_y_x(x1)) |>
    pivot_longer(cols = true:full, names_to = "model", values_to = "prediction") |>
    ggplot(aes(x1, prediction, group = simulation_run, col = model)) +
        geom_line(alpha = 0.3) +
        geom_line(aes(y = true_value), color = "black", linetype = "dashed") +
        facet_grid(n ~ model, labeller = label_both)
```

You can play along with this 
[Shiny App](https://divenyijanos.shinyapps.io/bias-variance-app/) 
which I put together to illustrate the bias-variance trade-off.