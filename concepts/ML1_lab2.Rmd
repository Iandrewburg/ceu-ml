---
title: "CEU Machine Learning Concepts - Lab 2"
author: János Divényi
output: html_notebook
---


```{r}
library(tidyverse)
library(glmnet)
theme_set(theme_minimal())
```

## Simulation

Let's continue with our model from last time.

```{r}
generateBeta <- function(beta_length) {
    4 / seq(beta_length)^2
}
f_y_x <- function(x) {
    beta <- generateBeta(dim(x)[2])  # approximately sparse model
    x %*% beta
}
```

```{r}
# Write a simulator function that can evaluate a model for a given X
# end result should be a data.frame with columns of lambda, prediction (f_hat), error
run_simulation <- function(x_generator, x0 = 0.1, lambdas = seq(0, 0.6, 0.02)) {
    # generate the sample
    x <- x_generator()
    y_exp <- f_y_x(x)
    y <- y_exp + rnorm(length(y_exp)) * 4

    # generate the x0 value
    x_eval <- matrix(x0, ncol = dim(x)[2])

    map_df(lambdas, ~{
        model <- glmnet(x, y, alpha = 1, lambda = .x)
        tibble(
            lambda = .x,
            f_hat = as.numeric(predict(model, newx = x_eval)),
            error = as.numeric(f_y_x(x_eval)) - f_hat
        )
    })
}
```

```{r}
visualize_simulation_results <- function(simulation_results) {
    group_by(simulation_results, lambda) |>
    summarise(bias2 = mean(error)^2, var = var(f_hat)) |>
    mutate(MSE = bias2 + var) |>
    pivot_longer(bias2:MSE, names_to = "metric") |>
    mutate(metric = factor(metric, levels = c("bias2", "var", "MSE"))) |>
    ggplot(aes(lambda, value, color = metric)) + geom_line(size = 1)
}
```


### Scenario #1: Independent predictors


```{r}
generate_independent_predictors <- function(n = 200, p = 100) {
    matrix(rnorm(n * p), nrow = n, ncol = p)
}
```


```{r}
# run simulation for n_sim times
run_simulation(x_generator = generate_independent_predictors)
n_sim <- 100
simulation_results <- map_df(
    seq(n_sim),
    run_simulation,
    x_generator = generate_independent_predictors
)
```


```{r}
visualize_simulation_results(simulation_results)
```


### Simulation scenario #2: Correlated predictors

The above model might seem unrealistic in most cases: you rarely have 100 independent predictors linearly affecting your outcome variable. However, you can use the same method to estimate more flexible models as well, e.g. by allowing for squares and interactions: with just 13 predictors you can end up with as much as 104 features which is very close to our previous model (only that now our features are correlated).

```{r}
generate_correlated_predictors <- function(n = 200, p = 13) {
    # for p = 13, there will be 104 features
    poly(matrix(rnorm(n * p), nrow = n, ncol = p), degree = 2, raw = TRUE)
}
```

```{r}
# run simulation for n_sim times
simulation_results <- map_df(
    seq(n_sim),
    run_simulation,
    x_generator = generate_correlated_predictors
)
```


```{r}
visualize_simulation_results(simulation_results)
```