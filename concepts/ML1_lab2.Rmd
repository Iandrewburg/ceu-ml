---
title: "CEU Machine Learning Concepts - Lab 2"
author: János Divényi
output: html_notebook
---


```{r}
library(tidyverse)
library(glmnet)
theme_set(theme_minimal())
```

## Simulation 

Let's continue with our model from last time.

```{r}
generateBeta <- function(beta_length) {
    4 / seq(beta_length)^2
}
f_y_x <- function(x) {
    beta <- generateBeta(dim(x)[2])  # approximately sparse model
    x %*% beta
}
```

```{r}
# Write a simulator function that can evaluate a model for a given X
# end result should be a data.frame with columns of lambda, prediction (f_star), error
run_simulation <- function(x0 = 0.1, lambdas = seq(0, 0.6, 0.02)) {
    # generate the sample
    n <- 200
    p <- 100
    x <- matrix(rnorm(n * p), nrow = n, ncol = p)	
    # y_exp <-
    y <- y_exp + rnorm(length(y_exp)) * 4
    
    # generate the x0 value
    x_eval <- matrix(x0, ncol = dim(x)[2])
    
    # (1) for each lambda
    # (2) evaluate at x_eval
    # (3) save the prediction and error into a tibble
    
}
```

```{r}
visualize_simulation_results <- function(simulation_results) {
    group_by(simulation_results, lambda) |>
    summarise(bias2 = mean(error)^2, var = var(f_star)) |>
    mutate(MSE = bias2 + var) |>
    pivot_longer(bias2:MSE, names_to = "metric") |>
    mutate(metric = factor(metric, levels = c("bias2", "var", "MSE"))) |>
    ggplot(aes(lambda, value, color = metric)) + geom_line(size = 1)    
}
```


### Scenario #1: Independent predictors


```{r}
generate_independent_predictors <- function(n = 200, p = 100) {
    matrix(rnorm(n * p), nrow = n, ncol = p)
}
```

Let's turn our `run_simulation` function flexible for any predictors: extract the feature generation into a function that could be passed to the simulator function. Thus, we can easily use the same simulator function for any types of features. Here

```{r}
# run simulation for n_sim times
# simulation_results <- 
```


```{r}
visualize_simulation_results(simulation_results)
```


### Simulation scenario #2: Correlated predictors

The above model might seem unrealistic in most cases: you rarely have 100 independent predictors linearly affecting your outcome variable. However, you can use the same method to estimate more flexible models as well, e.g. by allowing for squares and interactions: with just 13 predictors you can end up with as much as 104 features which is very close to our previous model (only that now our features are correlated).

```{r}
generate_correlated_predictors <- function(n = 200, p = 13) {
    # for p = 13, there will be 104 features
    poly(matrix(rnorm(n * p), nrow = n, ncol = p), degree = 2, raw = TRUE)
}
```

```{r}
# run simulation for n_sim times
# simulation_results <- 
```


```{r}
visualize_simulation_results(simulation_results)
```