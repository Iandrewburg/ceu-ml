---
title: "CEU Machine Learning Concepts - Lab 2"
author: "János K. Divényi"
output:
  html_document:
    df_print: paged
---

```{r setup, message=FALSE}
library(tidyverse)
library(caret)

options(scipen = 99)  # avoid scientific notation
#options(max.print = 20)  # avoid long outputs
#options(str = strOptions(list.len = 20))  # avoid long outputs
theme_set(theme_minimal())
```

# Evaluate by MSE

```{r model}
f_y_x <- function(x1) {
    x1^2 - 1.5 * x1
}
```

```{r generate-data}
n <- 100
# set.seed(20230213)

data <- tibble(
    x1 = runif(n),
    x2 = runif(n),
    y = f_y_x(x1) + rnorm(n)
)
```

```{r estimation}
estimated_models <- list(
    true = lm(y ~ x1 + I(x1^2), data),
    simple = lm(y ~ x1, data),
    full = lm(y ~ polym(x1, x2, degree = 2, raw = TRUE), data)
)
```

```{r evaluator-by-MSE}
# evaluate on multiple observations of a single dataset
calculateMSE <- function(prediction, y_exp) {
    mean((prediction - y_exp)^2)
}
```

```{r evaluation}
map(estimated_models, ~calculateMSE(predict(.x), f_y_x(data$x1)))
```

socrative.com
Student login: enter room “CEUML”

# Recap: caret

```{r estimate-lm-with-caret}
estimated_models_caret <- list(
    simple = train(
        y ~ x1, data, 
        method = "lm",
        trControl = trainControl(method = "none")
    ),
    full = train(
        y ~ polym(x1, x2, degree = 2, raw = TRUE), data, 
        method = "lm",
        trControl = trainControl(method = "none")
    )
)
```

```{r evaluation-caret}
map(estimated_models_caret, ~calculateMSE(predict(.x), f_y_x(data$x1)))
```

Let's create the _feature matrix_ beforehand (without the intercept as it will be included automatically in the model building phase).

```{r model-matrix}
expanded_data <- model.matrix(y ~ polym(x1, x2, degree = 2, raw = TRUE) + 0, data)
head(expanded_data)
```

!!!DANGER ALERT!!! Some exotic regex magic follows...
The functions below transform the names of our generated variables from `polym()` to some easier-to-digest alternatives.
No effect at all on what is calculated.

```{r tidy-var-names-functions}
tidyPolyVarNames <- function(var_string) {
    # handy tool for regexing in R: https://spannbaueradam.shinyapps.io/r_regex_tester/
    poly_str_match <- str_match(var_string, "polym\\((?<vars>.*), degree.*\\)(?<degrees>[0-9.]+)")
    vars <- str_extract_all(poly_str_match[, "vars"], "[A-z0-9]+")[[1]]
    degrees <- str_extract_all(poly_str_match[, "degrees"], "[0-9]")[[1]]
    map2(vars, degrees, ~powerToVarName(.x, .y)) |>
        compact() |>
        paste(collapse = "*")
}

powerToVarName <- function(var_name, power) {
    if (power == "0") {
        NULL
    } else if (power == "1") {
        var_name
    } else {
        paste(var_name, power, sep = "^")
    }
}
```

```{r tidy-var-names}
tidy_colnames <- map_chr(colnames(expanded_data), tidyPolyVarNames)
colnames(expanded_data) <- tidy_colnames
head(expanded_data)
```

```{r working-evaluation-caret}
estimated_models_caret <- list(
    simple = train(
        y ~ x1, data,
        method = "lm",
        trControl = trainControl(method = "none")
    ),
    full = train(
        x = expanded_data, y = data$y,
        method = "lm",
        trControl = trainControl(method = "none")
    )
)
map(estimated_models_caret, ~calculateMSE(predict(.x), f_y_x(data$x1)))
```

```{r compare-coefs}
coef(estimated_models$full)
coef(estimated_models_caret$full$finalModel)
```

# Brilliant idea: regularization

How do we know if we should use the simpler model? 
Which variables to use in the simpler model?
Solution: penalize complex models -> let the data choose the variables that matter (LASSO)



```{r lasso-with-no-penalty-is-lm}
full_glmnet <- train(
    x = expanded_data, y = data$y,
    method = "glmnet",
    trControl = trainControl(method = "none"),
    tuneGrid = expand.grid(alpha = 1, lambda = 0)
)
coef(full_glmnet$finalModel, 0)  # for some strange reason we have to add lambda = 0 again
coef(estimated_models_caret$full$finalModel)  # different optimization algorithms
```

```{r compare-mse-lm-lasso}
calculateMSE(predict(full_glmnet), f_y_x(data$x1))
calculateMSE(predict(estimated_models_caret$full), f_y_x(data$x1))
```




```{r first-lasso}
first_lasso <- train(
    x = expanded_data, y = data$y,
    method = "glmnet",
    trControl = trainControl(method = "none"),
    tuneGrid = expand.grid(alpha = 1, lambda = 0.1)
)
calculateMSE(predict(first_lasso), f_y_x(data$x1))
```


# Hyperparameter-tuning

```{r manual-hyperparam-tuning}
lambda_values <- seq(0, 0.5, 0.01)
results <- map_df(lambda_values, ~{
    model <- train(
        x = expanded_data, y = data$y,
        method = "glmnet",
        trControl = trainControl(method = "none"),
        tuneGrid = expand.grid(alpha = 1, lambda = .x)
    )
    mse <- calculateMSE(predict(model), f_y_x(data$x1))    
    tibble(lambda = .x, MSE = mse)
})
ggplot(results, aes(lambda, MSE)) + geom_line(linewidth = 1)
```

```{r auto-hyperparam-tuning}
caret_lasso <- train(
    x = expanded_data, y = data$y,
    method = "glmnet",
    tuneGrid = expand.grid(alpha = 1, lambda = lambda_values)
)
plot(caret_lasso)
plot(caret_lasso$finalModel, xvar = "lambda")
```


# Simulation

```{r simulation-function}
runSimulationStep <- function(n = 100, sd_e = 1, lambda = 0.01) {
    # Step 1: Simulate the data
    data <- tibble(
        x1 = runif(n),
        x2 = runif(n),
        y = f_y_x(x1) + rnorm(n, sd = sd_e)
    )
    
    expanded_data <- model.matrix(y ~ polym(x1, x2, degree = 2, raw = TRUE) + 0, data)
    tidy_colnames <- map_chr(colnames(expanded_data), tidyPolyVarNames)
    colnames(expanded_data) <- tidy_colnames
    
    # Step 2: Estimation
    estimated_models <- list(
        simple = train(
            y ~ x1, data,
            method = "lm",
            trControl = trainControl(method = "none")
        ),
        full = train(
            x = expanded_data, y = data$y,
            method = "lm",
            trControl = trainControl(method = "none")
        ),
        lasso = train(
            x = expanded_data, y = data$y,
            method = "glmnet",
            trControl = trainControl(method = "none"),
            tuneGrid = expand.grid(alpha = 1, lambda = lambda)
        )
    )
    
    lasso_coefs <- coef(estimated_models$lasso$finalModel, lambda)[, 1]
    
    # Step 3: Evaluation
    list(
        mse = map(estimated_models, ~calculateMSE(predict(.x), f_y_x(data$x1))),
        lasso_nonzero = lasso_coefs[lasso_coefs != 0]
    )
    
}
```

```{r sample-simulation}
runSimulationStep()
```

```{r mc-simulation}
set.seed(20230213)

n_sim <- 1000
simulated_results <- map(seq(n_sim), ~runSimulationStep())
```

```{r mse-table}
pivot_longer(map_df(simulated_results, "mse"), cols = everything()) |>
    group_by(name) |>
    summarise(
        avg = mean(value),
        median = median(value),
        q95 = quantile(value, 0.95)
    )
```

```{r how-many-coefs-are-nonzero}
nonzero_coefs <- map(simulated_results, "lasso_nonzero") |>
    map(tail, -1)
map_dbl(nonzero_coefs, length) |>
    table()
```

```{r which-coefs-are-nonzero}
map(nonzero_coefs, names) |>
    unlist() |>
    table()
```
