---
title: "CEU Machine Learning Concepts - Lab 3"
author: "János K. Divényi"
output:
  html_document:
    df_print: paged
---

```{r setup, message=FALSE}
library(tidyverse)
library(caret)
library(glmnet)

options(scipen = 99)  # avoid scientific notation
theme_set(theme_minimal())
```

# Recap: LASSO

```{r model}
f_y_x <- function(x1) {
    x1^2 - 1.5 * x1
}
```

```{r generate-data}
n <- 100
set.seed(20230220)

data <- tibble(
    x1 = runif(n),
    x2 = runif(n),
    y = f_y_x(x1) + rnorm(n)
)
```

```{r calculate-mse}
calculateMSE <- function(prediction, y_exp) {
    mean((prediction - y_exp)^2)
}
```

```{r feature-matrix}
# called extended_data in lab2
feature_matrix <- model.matrix(y ~ polym(x1, x2, degree = 2, raw = TRUE) + 0, data)
```

```{r caret}
caret_lasso <- train(
    x = feature_matrix, y = data$y,
    method = "glmnet",
    trControl = trainControl(method = "none"),
    tuneGrid = expand.grid(alpha = 1, lambda = 0.1)
)
calculateMSE(predict(caret_lasso), f_y_x(data$x1))
```

```{r simple-glmnet}
base_lasso <- glmnet(
    x = feature_matrix, y = data$y,
    alpha = 1, lambda = 0.1
)
# caret predicts on the original data by default -- glmnet needs an explicit `newx` parameter
calculateMSE(predict(base_lasso, feature_matrix), f_y_x(data$x1))
```

```{r simple-glmnet-with-multiple-lambdas}
lambda_values <- seq(0, 0.2, 0.01)
lasso_models <- glmnet(
    x = feature_matrix, y = data$y,
    alpha = 1, lambda = lambda_values
)
lasso_models
```
```{r predict-with-glmnet}
# we get a list of prediction for each lambda value
predict(lasso_models, feature_matrix)
```
```{r coefficients-from-glmnet}
lasso_models$beta
```


```{r number-of-nonzero-coefficients}
lasso_models$df
```


```{r predict-on-a-given-point}
point_to_evaluate <- c(x1 = 0, x2 = 0)
predict(lasso_models, point_to_evaluate)
```


# Bias-variance simulation by lambda

```{r simulation-function}
runSimulationStep <- function(n = 100, sd_e = 1, lambda_values = seq(0, 0.2, 0.01)) {
    # Step 1: Simulate the data
    data <- tibble(
        x1 = runif(n),
        x2 = runif(n),
        y = f_y_x(x1) + rnorm(n, sd = sd_e)
    )

    feature_matrix <- model.matrix(y ~ polym(x1, x2, degree = 2, raw = TRUE) + 0, data)
    
    # Step 2: Estimation
    lasso_models <- glmnet(
        x = feature_matrix, y = data$y,
        alpha = 1, lambda = lambda_values
    )

    # Step 3: Prediction
    tibble(
        lambda = round(lasso_models$lambda, digits = 2),  # rounding is needed to ensure that the lambda values remain the same across runs (glm implementation is buggy)
        predictions = as.numeric(predict(lasso_models, point_to_evaluate)),
        n_nz_coefs = lasso_models$df
    )
}
```

```{r sample-simulation}
runSimulationStep()
```

```{r mc-simulation}
set.seed(20230220)

n_sim <- 1000
simulated_results <- map_df(seq(n_sim), ~runSimulationStep())
```

```{r visualize-bias-variance}
group_by(simulated_results, lambda) |>
    summarise(
        bias2 = (mean(predictions) - f_y_x(0))^2,
        var = var(predictions),
        MSE = bias2 + var
    ) |>
    pivot_longer(cols = bias2:MSE) |>
    ggplot(aes(lambda, value, col = name)) +
    geom_line(linewidth = 1)
```

```{r number-of-nonzero-coefficients-by-lambda}
group_by(simulated_results, lambda) |>
    summarise(n_nz_coefs = mean(n_nz_coefs)) |>
    ggplot(aes(lambda, n_nz_coefs)) + 
        geom_line(linewidth = 1) +
        
```


## Principal Component Analysis (PCA)

```{r}
library(factoextra)
```


We transform the coordinates of the original variables to capture as much variation as we can with independent (orthogonal) dimensions.
For a very nice illustration and discussion, see [here](https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues).

```{r}
data(decathlon2)
decathlon2
```


## An example with two variables

Take two variables first only. We seek a linear combination of them that
* has the highest possible variance
* with weights that are normalized (their sum of squares are 1)

```{r}
short_runs <- select(decathlon2, X100m, X400m)
ggplot(decathlon2, aes(X100m, X400m)) + geom_point()
```

```{r}
# the variables have totally different variances
summarise(short_runs, across(everything(), var))
```

Demean the data only for easier visualizations.
```{r}
short_runs_demeaned <- mutate(short_runs, across(everything(), ~ .x - mean(.x)))
```

```{r}
ggplot(short_runs_demeaned, aes(X100m, X400m)) +
    geom_point() +
    coord_fixed()
```

The goal is to find a linear combination of the two variables that captures most of the joint variance. Indeed, we see that we get back the weight we obtained from the `prcomp` function.

```{r}
# constraint: w_100m^2 + w_400m^2 = 1
# this means that w_100m = sqrt(1 - w_400m^2)

objective <- function(w_100m) {
    # we want to maximize variance
    # minus: since "optim" applies minimization.
    -var(
        w_100m * short_runs_demeaned$X100m +
        sqrt(1 - w_100m^2) * short_runs_demeaned$X400m
    )
}

optim_result <- optimize(f = objective, interval = c(0, 1), tol = 1e-15)
w_100m <- optim_result$minimum
w_400m <- sqrt(1 - w_100m^2)
message(glue::glue("Weight of 100m: {round(w_100m, 7)} \n Weight of 400m: {round(w_400m, 7)}"))
```

With PCA we can arrive at the same result.

```{r}
# Note from the help of prcomp:
# "The signs of the columns of the rotation matrix are arbitrary, and so may differ between different programs for PCA, and even between different builds of R."
pca_short_run <- prcomp(short_runs_demeaned)
pca_short_run
```

```{r}
pc1 <- pca_short_run$rotation[, "PC1"]
pc1
```

Let us depict this variance-maximizing linear combination of the two variables
in the space of the original variables.
```{r}
ggplot(short_runs_demeaned, aes(X100m, X400m)) +
    geom_point() +
    coord_fixed() +
    geom_abline(slope = pc1[["X400m"]] / pc1[["X100m"]], color = "red")
```

WARNING: this line is very different from regressing X100m on X400m! PCA's aim is to find a line to which if
observations are projected, variance is the highest. Regression: squared errors to be minimized.

```{r}
ggplot(short_runs_demeaned, aes(X100m, X400m)) +
    geom_point() +
    coord_fixed() +
    geom_abline(slope = pc1[["X400m"]] / pc1[["X100m"]], color = "red") +
    geom_abline(slope = coef(lm(X400m ~ X100m, data = short_runs_demeaned))[["X100m"]], color = "blue")
```

See more about it [here](https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues/140579#140579).

## The full example

Scaling: necessary to have comparable units of different variables.
(Multiplying a variable by 100 increases the variance and PCA would try to capture that, even though it is only an artifact of units. We don't want that.)
In the previous example we did not scale all variables for the sake of nicer illustrations and we indeed saw that the variable with the higher absolute variance got a lot more weight than the other. This is not really what we are after.

Let's perform PCA on all the variables. The first principal component shows weights for a linear combination of the original variables that maximizes variance (up to constraining weights to have a sum of square equal to 1).
```{r}
results10 <- select(decathlon2, 1:10)
pca_result <- prcomp(results10, scale. = TRUE)
summary(pca_result)
```



Let's check if our numbers are corresponding to our expectations:

- Weights sum to 1

```{r}
pca_result$rotation
colSums(pca_result$rotation^2)
```

- PC-s are orthogonal to each other: they contain "independent" variance from the data (we take the first and the last principal component as an illustration - the orthogonality condition should hold for each possible pairs)

```{r}
sum(pca_result$rotation[, 1] * pca_result$rotation[, 10])
```

- if we take the linear combination of (scaled) original variables with the weights specified by PC1, we get back the standard deviation of PC1

```{r}
pc1_loadings <- pca_result$rotation[, "PC1"]
pc1_value_for_observations <- scale(results10) %*% pc1_loadings  # %*%: matrix-vector product
sd(pc1_value_for_observations)
```

- the total variance equals to the number of variables (due to the scaling)

```{r}
sum(summary(pca_result)$importance[1,]^2)
```

Let's examine our PCA result using the `factoextra::fviz_pca()` function that plots observations as well as original features in the space spanned
by the first two principal components.

```{r}
fviz_pca(pca_result)
```

## PCA on gene data

From the ISLR website, we can download a gene expression data set (Ch10Ex11.csv) that consists of 40 tissue samples with measurements on 1,000 genes. The first 20 samples are from healthy patients, while the second 20 are from a diseased group.

```{r}
genes <- read_csv("https://www.statlearning.com/s/Ch10Ex11.csv", col_names = FALSE) %>%
    t() %>% as_tibble()
dim(genes)

health_status <- c(rep("healthy", 20), rep("diseased", 20))
```


```{r}
# Run PCA on genes data and look at the first ten principal components
genes_pca <- prcomp(genes, rank. = 10)
summary(genes_pca)
```

Let's visualize the first two principal components. You can extract the principal component scores of your features by using our old friend, the `predict()` function.

```{r}
predict(genes_pca) |>
    as_tibble() |>
    mutate(health_status = health_status) |>
    ggplot(aes(PC1, PC2, col = health_status)) +
        geom_vline(xintercept = 0, linetype = "dashed") +
        geom_hline(yintercept = 0, linetype = "dashed") +
        geom_point(size = 3, alpha = 0.6)
```

The first principal component contains all the relevant information we need here. Instead of having to look at 1,000 features it is enough to look at this simple chart.


