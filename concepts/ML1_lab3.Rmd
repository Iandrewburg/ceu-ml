---
title: "CEU Machine Learning Concepts - Lab 3"
author: János Divényi
output: html_notebook
---

## Principal Component Analysis (PCA)

```{r}
library(tidyverse)
library(glmnet)
library(factoextra)
theme_set(theme_minimal())
```


We transform the coordinates of the original variables to capture as much
variation as we can with independent (orthogonal) dimensions.
For a very nice illustration and discussion, see [here](https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues).

```{r}
data(decathlon2)
decathlon2
```


## An example with two variables

Take two variables first only. We seek a linear combination of them that
* has the highest possible variance
* with weights that are normalized (their sum of squares are 1)

```{r}
short_runs <- select(decathlon2, X100m, X400m)
ggplot(decathlon2, aes(X100m, X400m)) + geom_point()
```

```{r}
# the variables have totally different variances
summarise(short_runs, across(everything(), var))
```

Demean the data only for easier visualizations.
```{r}
short_runs_demeaned <- mutate(short_runs, across(everything(), ~ .x - mean(.x)))
```

```{r}
ggplot(short_runs_demeaned, aes(X100m, X400m)) +
    geom_point() +
    coord_fixed()
```

The goal is to find a linear combination of the two variables that captures most of the joint
variance. Indeed, we see that we get back the weight we obtained from the `prcomp` function.

```{r}
# constraint: w_100m^2 + w_400m^2 = 1
# this means that w_100m = sqrt(1 - w_400m^2)

objective <- function(w_100m) {
    # we want to maximize variance
    # minus: since "optim" applies minimization.
    -var(
        w_100m * short_runs_demeaned$X100m +
        sqrt(1 - w_100m^2) * short_runs_demeaned$X400m
    )
}

optim_result <- optimize(f = objective, interval = c(0, 1), tol = 1e-15)
w_100m <- optim_result$minimum
w_400m <- sqrt(1 - w_100m^2)
message(glue::glue("Weight of 100m: {round(w_100m, 7)} \n Weight of 400m: {round(w_400m, 7)}"))
```

With PCA we can arrive at the same result.

```{r}
# Note from the help of prcomp:
# "The signs of the columns of the rotation matrix are arbitrary, and so may differ between different programs for PCA, and even between different builds of R."
pca_short_run <- prcomp(short_runs_demeaned)
pca_short_run
```

```{r}
pc1 <- pca_short_run$rotation[, "PC1"]
pc1
```

Let us depict this variance-maximizing linear combination of the two variables
in the space of the original variables.
```{r}
ggplot(short_runs_demeaned, aes(X100m, X400m)) +
    geom_point() +
    coord_fixed() +
    geom_abline(slope = pc1[["X400m"]] / pc1[["X100m"]], color = "red")
```

WARNING: this line is very different from regressing X100m on X400m! PCA's aim is to find a line to which if
observations are projected, variance is the highest. Regression: squared errors to be minimized.

```{r}
ggplot(short_runs_demeaned, aes(X100m, X400m)) +
    geom_point() +
    coord_fixed() +
    geom_abline(slope = pc1[["X400m"]] / pc1[["X100m"]], color = "red") +
    geom_abline(slope = coef(lm(X400m ~ X100m, data = short_runs_demeaned))[["X100m"]], color = "blue")
```

See more about it [here](https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues/140579#140579).

## The full example

Scaling: necessary to have comparable units of different variables.
(Multiplying a variable by 100 increases the variance and PCA would try to
capture that, even though it is only an artifact of units. We don't want that.)
In the previous example we did not scale all variables for the sake of nicer illustrations and we indeed saw that the variable with the higher absolute variance got a lot more weight than the other. This is not really what we are after.

Let's perform PCA on all the variables. The first principal component shows weights for a linear combination of the original variables that maximizes variance (up to constraining weights to have a sum of square equal to 1).
```{r}
results10 <- select(decathlon2, 1:10)
pca_result <- prcomp(results10, scale. = TRUE)
summary(pca_result)
```



Let's check if our numbers are corresponding to our expectations:

- Weights wum to 1

```{r}
pca_result$rotation
colSums(pca_result$rotation^2)
```

- PC-s are orthogonal to each other: they contain "independent" variance from the data

```{r}
sum(pca_result$rotation[, 1] * pca_result$rotation[, 2])
```

- if we take the linear combination of (scaled) original variables with the weights specified by
PC1, we get back the standard deviation of PC1

```{r}
pc1_loadings <- pca_result$rotation[, "PC1"]
pc1_value_for_observations <- scale(results10) %*% pc1_loadings  # %*%: matrix-vector product
sd(pc1_value_for_observations)
```

- the total variance equals to the number of variables (due to the scaling)

```{r}
total_variance <- sum(variances)
total_variance
```

Let's examine our PCA result using the `factoextra::fviz_pca()` function that plots observations as well as original features in the space spanned
by the first two principal components.

```{r}
fviz_pca(pca_result)
```



## PCA on our previous data

Let's go back to our last week's data, and do PCA on them.

```{r}
generate_independent_predictors <- function(n = 200, p = 100) {
    matrix(rnorm(n * p), nrow = n, ncol = p)
}
generate_correlated_predictors <- function(n = 200, p = 13) {
    poly(matrix(rnorm(n * p), nrow = n, ncol = p), degree = 2, raw = TRUE)
}
```

What would you expect on the independent predictor case? Can PCA summarize the data?

```{r}
x_indep <- generate_independent_predictors()
pca_indep <- prcomp(x_indep)
summary(pca_indep)
pc_variances <- sort(pca_indep$sdev^2, decreasing = TRUE)
qplot(
    seq_along(pc_variances), pc_variances / sum(pc_variances), 
    xlab = "PC", ylab = "Variance explained"
) + geom_hline(yintercept = 1 / length(pc_variances), linetype = "dotted")
```

```{r}
# The first m PC-s that summarizes ~95% of the total variance
cum_prop <- summary(pca_indep)$importance["Cumulative Proportion",]
cum_prop[which(cum_prop < 0.95)]
```

It seems as if it could. However: we know that the original features were already orthogonal _by design_. What happened?
The "true" features are orthogonal, but their sample representations are not.

```{r}
sample_correlation_matrix <- cor(x_indep)
sort(sample_correlation_matrix[, 1], decreasing = TRUE)
sort(sample_correlation_matrix[, 45], decreasing = TRUE)
```

We fitted PCA on the noise, and got spurious results.
Always keep in mind that our numbers reflect estimations nut true values. This is why we look at confidence intervals or do resamplings.
This example also gave you a sense of how much "info" you can find in a random sample.

Now let's look at our other case when the predictors are somewhat correlated.

```{r}
x_dep <- generate_correlated_predictors()
pca_dep <- prcomp(x_dep)
summary(pca_dep)
```

The correlation is not high but we did not expect it as worked with products instead of linear combinations.
Still, we might use PCA to reduce the dimensionality of our original data set. It might help to use this data to predict outcomes.

```{r}
# Use our previous approximately sparse model
generateBeta <- function(beta_length) {
    4 / seq(beta_length)^2
}
f_y_x <- function(x) {
    beta <- generateBeta(dim(x)[2])
    x %*% beta
}
y_exp <- f_y_x(x_dep)
y <- y_exp + rnorm(length(y_exp)) * 4
```



```{r}
calculateMSE <- function(prediction, y_exp) {
    mean((prediction - y_exp)^2)
}
# benchmark
simple_linreg <- glmnet(x_dep, y, alpha = 1, lambda = 0)
calculateMSE(predict(simple_linreg, newx = x_dep), y_exp)
```

How many components summarize the data? No definite answer: decide based on sufficient variance explained.

```{r}
# PCA
pcr_model <- pcr(y ~ x_dep)
summary(pcr_model)
calculateMSE(as.numeric(predict(pcr_model, newx = x_dep, ncomp = 74)), y_exp)
```

Principal Component Regression (PCR) could achieve some gain on prediction performance even for this data set where the features are relatively independent.
LASSO works much better, though. Not surprising: PCA does not take into account in any way the relation between the features and the outcome.

```{r}
simple_lasso <- glmnet(x_dep, y, alpha = 1, lambda = 0.1)
calculateMSE(predict(simple_lasso, newx = x_dep), y_exp)
```

## PCA on gene data

From the ISLR website, we can download a gene expression data set (Ch10Ex11.csv) that consists of 40 tissue samples with measurements on 1,000 genes. The first 20 samples are from healthy patients, while the second 20 are from a diseased group.

```{r}
genes <- read_csv("https://www.statlearning.com/s/Ch10Ex11.csv", col_names = FALSE) %>%
    t() %>% as_tibble()
dim(genes)

health_status <- c(rep("healthy", 20), rep("diseased", 20))
```


```{r}
# Run PCA on genes data and look at the first two principal components

```



