---
title: "CEU Machine Learning Tools - Session 2"
author: János Divényi
output: html_notebook
---

## Classification problem with h2o: Prostate cancer

H2O: state-of-the-art machine learning software that is even suitable for big datasets.
It offers very efficient and scalable implementations of popular ML algorithms that can

* run on distributed systems
* utilize multiple cores
* work with GPUs

Models estimated with h2o can be deployed to production environments
through Java objects (see [here](http://docs.h2o.ai/h2o/latest-stable/h2o-docs/productionizing.html)).
Also, h2o scales well compared to other competitor implementations
(see Szilard Pafka's famous benchmarks [here](https://github.com/szilard/benchm-ml)).

In general, best resource to learn is the
[documentation](http://docs.h2o.ai/h2o/latest-stable/h2o-docs/welcome.html) and many
tutorials are available on YouTube.
```{r libs}
library(tidyverse)
theme_set(theme_minimal())
library(h2o)
h2o.init()
```

We are going to work with prostate data collected by Dr. Donn Young at The Ohio State University Comprehensive Cancer Center. The goal of the analysis is to determine whether variables measured at a baseline exam can be used to predict whether the tumor has penetrated the prostate capsule.
The data presented are a subset of variables from the main study with CAPSULE acting as the ground truth:

* ID: A row identifier. This can be dropped from the list of predictors.
* CAPSULE: Whether the tumor penetrated the prostatic capsule
* AGE: The patient’s age
* RACE: The patient’s race
* DPROS: The result of the digital rectal exam, where 1=no nodule; 2=unilober nodule on the left; 3 =unilibar nodule on the right; and 4=bilobar nodule.
* DCAPS: Whether there existed capsular involvement on the rectal exam
* PSA: The Prostate Specific Antigen Value (mg/ml)
* VOL: The tumor volume (cm3)
* GLEASON: The patient’s Gleason score in the range 0 to 10

```{r data-manipulation}
# import the prostate dataset
data_url <- "https://h2o-public-test-data.s3.amazonaws.com/smalldata/prostate/prostate.csv"
prostate_data <- h2o.importFile(data_url)

# convert columns to factors (we cannot use the tidyverse functions here as the object is not a normal df)
prostate_data$CAPSULE <- as.factor(prostate_data$CAPSULE)
prostate_data$RACE <- as.factor(prostate_data$RACE)
prostate_data$DCAPS <- as.factor(prostate_data$DCAPS)
prostate_data$DPROS <- as.factor(prostate_data$DPROS)

summary(prostate_data)
```

```{r data-split}
my_seed <- 20220316
prostate_data_splits <- h2o.splitFrame(data =  prostate_data, ratios = 0.8, seed = my_seed)
train <- prostate_data_splits[[1]]
test <- prostate_data_splits[[2]]
```

## Basic models

```{r avg-benchmark}
# Calculate avg for benchmark
mean(train$CAPSULE)
```

```{r estimate-logit}
# set the predictor and response columns
response <- "CAPSULE"
predictors <- setdiff(names(prostate_data), c(response, "ID"))

# build a simple GLM model using CV (just for evaluation, no hyperparam tuning yet)
simple_logit <- h2o.glm(
    family = "binomial",
    model_id = "logit",
    x = predictors,
    y = response,
    training_frame = train,
    lambda = 0,
    nfolds = 5
)
```

```{r look-at-h2o-fit-object}
simple_logit
```

```{r predict-h2o}
logit_predictions <- h2o.predict(object = simple_logit, newdata = test)
head(logit_predictions)
```

```{r evaluate-performance}
logit_performance <- h2o.performance(simple_logit, xval = TRUE)
h2o.accuracy(logit_performance, thresholds)
plot(logit_performance, type = "roc")
plot(logit_performance, type = "pr")
h2o.confusionMatrix(logit_performance)
h2o.confusionMatrix(logit_performance, threshold = 0.5)
```


```{r tree}
# no dedicated single decision tree algorithm, so run a restricted randomForest:
#  ntrees = 1, mtries = k and sample_rate = 1 (grow one tree on all the columns using all obs)
simple_tree <- h2o.randomForest(
    predictors,
    response,
    training_frame = train,
    model_id = "tree",
    ntrees = 1, mtries = length(predictors), sample_rate = 1,
    nfolds = 5,
    seed = my_seed
)
```

```{r print-tree}
tree_model <- h2o.getModelTree(simple_tree, 1)  # gets the first tree which is the only one now
cat(tree_model@tree_decision_path)
```

```{r evaluate-tree}
h2o.accuracy(h2o.performance(simple_tree)) # for RF, h2o tries to give bag OOB errors
h2o.accuracy(h2o.performance(simple_tree, xval = TRUE))
h2o.predict(simple_tree, test)
```

```{r random-forest}
# Estimate a random forest model with the default settings and with my_seed
# Calculate the accuracy for cv data at the 0.5 cutoff
# Plot the ROC curve evaluated on the cv data

```

## Comparison of models using ROC / PR plots

The default ROC plot is nice but is only capable of showing one model. If we want to compare multiple models on the same chart, we should calculate the metrics for varying thresholds. You might implement this using a simple loop on the predictions. Or you can just extract the calculated metrics from the `h2o.performance` object.

```{r extract-metrics}
getMetricsWithThresholds <- function(model, newdata = NULL, xval = FALSE) {
    h2o.performance(model, newdata = newdata, xval = xval)@metrics$thresholds_and_metric_scores |>
        as_tibble() |>
        mutate(model = model@model_id)
}
logit_metrics <- getMetricsWithThresholds(simple_logit, xval = TRUE)
logit_metrics
```

Plot ROC curve
```{r plot-roc}
plotROC <- function(metrics_df) {
    ggplot(metrics_df, aes(fpr, tpr, color = model)) +
        geom_path() +
        geom_abline(slope = 1, intercept = 0, linetype = "dashed") +
        coord_fixed() +
        labs(x = "False Positive Rate", y = "True Positive Rate")
}
plotROC(logit_metrics)
```

Plot RP curve
```{r plot-rp}
plotRP <- function(metrics_df) {
    ggplot(metrics_df, aes(precision, tpr, color = model)) +  # tpr = recall
        geom_line() +
        labs(x = "Precision", y = "Recall (TPR)")
}
plotRP(logit_metrics)
```

```{r compare-models}
simple_models <- list(simple_logit, simple_tree)
simple_models_performance <- map_df(simple_models, getMetricsWithThresholds, xval = TRUE)
plotROC(simple_models_performance)
plotRP(simple_models_performance)
```

## Hyper-parameter tuning

```{r rf-grid-search}
rf_params <- list(
    ntrees = c(10, 50, 100, 300),
    mtries = c(2, 4, 6),
    sample_rate = c(0.2, 0.632, 1),
    max_depth = c(5, 10, 20)
)

rf_grid <- h2o.grid(
    "randomForest", x = predictors, y = response,
    training_frame = train,
    grid_id = "rf",
    nfolds = 5,  # the optimization could be done on OOB samples as well
    seed = my_seed,
    hyper_params = rf_params,
    keep_cross_validation_predictions = TRUE   # needed for stacking them later
)
```

```{r look-at-tuned-results}
# Note that if neither cross-validation nor a validation frame is used in the grid search, then the training metrics will display in the "get grid" output. If a validation frame is passed to the grid, and nfolds = 0, then the validation metrics will display. However, if nfolds > 1, then cross-validation metrics will display even if a validation frame is provided.
h2o.getGrid(rf_grid@grid_id, "auc")
```

```{r plot-tuned-results}
rf_performance_summary <- h2o.getGrid(rf_grid@grid_id, "auc", decreasing = TRUE)@summary_table %>%
    as_tibble() %>%
    mutate(across(c("auc", names(rf_params)), as.numeric))
ggplot(rf_performance_summary, aes(ntrees, auc, color = factor(mtries))) +
    geom_line() +
    facet_grid(max_depth ~ sample_rate, labeller = label_both) +
    theme(legend.position = "bottom") +
    labs(color = "mtry")
```

```{r compare-ROC}
rf_metrics <- map_df(rf_grid@model_ids, ~{
    getMetricsWithThresholds(h2o.getModel(.), xval = TRUE)
})

ggplot(rf_metrics, aes(fpr, tpr, group = model)) +
    geom_path(alpha = 0.2) +
    geom_abline(slope = 1, intercept = 0, linetype = "dashed") +
    coord_fixed() +
    labs(x = "False Positive Rate", y = "True Positive Rate")
```
```{r stacking}
ensemble_model_grid_rf <- h2o.stackedEnsemble(
    x = predictors, y = response,
    training_frame = train,
    metalearner_algorithm = "glm",  # could try others as well, e.g. "gbm"
    base_models = rf_grid@model_ids
)
ensemble_model_grid_rf
ensemble_model_grid_rf@model$metalearner_model@model$coefficients_table
```

```{r roc-with-ensemble}
ensemble_performance <- getMetricsWithThresholds(ensemble_model_grid_rf, xval = TRUE)

ggplot(rf_metrics, aes(fpr, tpr, group = model)) +
    geom_path(alpha = 0.2) +
    geom_path(color = "firebrick", data = ensemble_performance) +
    geom_abline(slope = 1, intercept = 0, linetype = "dashed") +
    coord_fixed() +
    labs(x = "False Positive Rate", y = "True Positive Rate")
```


## XGBoost

A celebrated implementation of the gradient boosting idea.

> "Both xgboost and gbm follows the principle of gradient boosting. There are however, the difference in modeling details. Specifically, xgboost used a more regularized model formalization to control over-fitting, which gives it better performance." (https://xgboost.readthedocs.io/en/latest/tutorials/model.html)

```{r simple-xgboost}
simple_xgboost <- h2o.xgboost(
    x = predictors, y = response,
    model_id = "simple_xgboost",
    training_frame = train,
    validation_frame = test,
    nfolds = 5,
    score_each_iteration = TRUE,
    seed = my_seed
)
simple_xgboost
```

```{r xgboost-score-history}
xgboost_score_history <- h2o.scoreHistory(simple_xgboost)
pivot_longer(xgboost_score_history, ends_with("error")) |>
    ggplot(aes(number_of_trees, value, color = name)) +
    geom_line()
```


```{r xgboost-param-tuning}
# Tune the following hyperparams trying at least 2 values for each: learn_rate, ntrees, sample_rate, gamma (regularization param) and plot the AUC for each combination
xgboost_params <- list(
    learn_rate = ,  # default: 0.3
    ntrees = ,
    max_depth = ,
    gamma =   # regularization parameter: min_split_improvement, default: 0
)

```


## AutoML

H2O has a built-in autoML feature that can do all the tuning and experimenting for you.
Can it do better than we did with blood, sweat and tears?

```{r automl}
automl <- h2o.automl(
    x = predictors, y = response,
    training_frame = train,
    nfolds = 5,
    sort_metric = "AUC",
    seed = my_seed,
    max_runtime_secs = 60 # limit the run-time
)
automl
h2o.auc(h2o.performance(automl@leader, xval = TRUE))
h2o.model_correlation_heatmap(automl, xval = TRUE)
```


## Classify fashion images

The Fashion-MNIST dataset consists of Zalando’s article images. Each example is a 28×28 grayscale image, associated with a label from 10 classes:

0. T-shirt/top
1. Trouser
2. Pullover
3. Dress
4. Coat
5. Sandal
6. Shirt
7. Sneaker
8. Bag
9. Ankle boot

```{r load-fmnist}
fmnist_data <- read_csv("../data/fashion/fmnist5000.csv")
fmnist_data <- mutate(fmnist_data, label := as.factor(label))
```

```{r plot-fashion-data}
xy_axis <- as_tibble(expand.grid(x = 1:28, y = 28:1))
plot_theme <- list(
    raster = geom_raster(hjust = 0, vjust = 0),
    gradient_fill = scale_fill_gradient(low = "white", high = "black", guide = "none"),
    theme = theme(
        axis.line = element_blank(),
        axis.text = element_blank(),
        axis.ticks = element_blank(),
        axis.title = element_blank(),
        panel.background = element_blank(),
        panel.border = element_blank(),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        plot.background = element_blank()
    )
)

id <- 2
cbind(xy_axis, fill = t(fmnist_data[id, -1])) |>
    ggplot(aes(x, y, fill = fill)) + plot_theme + labs(title = fmnist_data[id, 1])
```
```{r create-h2o-frames}
# I assign only 20% to training to shorten the training time
data_split <- h2o.splitFrame(as.h2o(fmnist_data), ratios = 0.2, seed = my_seed)
fmnist_train <- data_split[[1]]
fmnist_holdout <- data_split[[2]]
```


```{r simple-dl-on-fashion}
fmnist_dl_default <- h2o.deeplearning(
    x = 2:785,
    y = "label",
    training_frame = fmnist_train,
    validation_frame = fmnist_holdout,
    model_id = "fmnist_dl_default",
)
```

```{r investigate-h2o-dl}
fmnist_dl_default
h2o.scoreHistory(fmnist_dl_default)
plot(fmnist_dl_default, metric = "classification_error")
```

```{r look-at-dl-params}
params <- fmnist_dl_default@allparameters
str(params)
?h2o.deeplearning
```


```{r adjusted-dl-on-fashion}
fmnist_dl_adjusted <- h2o.deeplearning(
    x = 2:785,
    y = "label",
    training_frame = fmnist_train,
    validation_frame = fmnist_holdout,
    model_id = "fmnist_dl_adjusted",
    hidden = c(20, 20),
    mini_batch_size = 20,
    score_each_iteration = TRUE
)
```