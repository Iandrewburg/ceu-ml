---
title: "CEU Machine Learning Tools - Session 4"
author: János Divényi
output: html_notebook
---

## Assessing Credit Risk

```{r libs}
library(tidyverse)
library(h2o)
library(DALEX)
library(DALEXtra)
theme_set(theme_minimal())
```

We are going to assess credit risk using a dataset on credit applications made by German citizens. The cleaned version of this classic dataset is available on [Kaggle](https://www.kaggle.com/datasets/kabure/german-credit-data-with-risk) has a total of 1000 instances with a range of features such as age, employment status, credit history, loan amount, and purpose of the loan (see more detail about the features [here](https://www.kaggle.com/datasets/uciml/german-credit)). By analyzing the patterns and trends in the dataset, financial institutions can make informed decisions about whether to approve or reject credit applications and manage their risk effectively.


```{r credit-risk-data}
credit_data <- read_csv("../data/german_credit_risk/german_credit_data.csv")
skimr::skim(credit_data)
```

```{r data-manipulation}
credit_data <- select(credit_data, -1) |>  # drop the ID column
    rename_with(~tolower(gsub(" ", "_", .x, fixed = TRUE))) |>
    mutate(
        risk = as.factor(ifelse(risk == "bad", 1L, 0L)),
        job = case_when(
            job == 0 ~ "unskilled and non-resident", 
            job == 1 ~ "unskilled and resident", 
            job == 2 ~ "skilled", 
            job == 3 ~ "highly skilled"
        ),
        across(c(saving_accounts, checking_account), ~ifelse(is.na(.x), "missing", .x)),
        across(where(is.character), as.factor)
    )
skimr::skim(credit_data)
```


```{r data-on-h2o}
h2o.init()
my_seed <- 20230329
data_split <- h2o.splitFrame(as.h2o(credit_data), ratios = 0.75, seed = my_seed)
credit_train <- data_split[[1]]
credit_holdout <- data_split[[2]]

target <- "risk"
features <- setdiff(names(credit_data), target)
```

```{r train-model}
# Train the best classification model you can come up with (optimize for AUC)
```


```{r evaluate-our-model}
h2o.performance(my_model, xval = TRUE)
h2o.confusionMatrix(my_model, xval = TRUE)
```


```{r variable-importance-in-h2o}
h2o.varimp(my_model)
h2o.varimp_plot(my_model)
```
```{r h2o-explain}
# gives useful information: residual analysis, variable importance, SHAP Summary, PDP-s -- but hard to customize
h2o.explain(my_model, credit_holdout)
```
## Diagnostics with Dalex

```{r create-dalex-explainer}
dalex_explainer <- explain_h2o(my_model, data = credit_holdout[features], y = as.numeric(credit_holdout[[target]]))
class(dalex_explainer)
summary(dalex_explainer)
```
### Variable importance

Dalex calculates permutation-based feature importance (model-agnostic). The parameter `B` stands for the number of permutations. It shows how much the model's performance would change if we did not include the given variable.

```{r variable-importance-with-dalex}
dalex_vip <- model_parts(dalex_explainer, B = 20)  # takes a while
plot(dalex_vip)
```
### Partial Depedence Plots

Partial Dependence Plots express how the change of a feature influences the prediction.

PDP is sensitive to correlated features. Accumulated Local Effect expresses the similar idea but in a more robust way. See chapters 17-18 of Biecek-Burzykowski for more detail.

```{r partial-dependence-plot-age}
pdp_age <- model_profile(dalex_explainer, variables = "age", type = "partial")  # default: partial
plot(pdp_age, geom = "profiles")

pdp_age_accumulated <- model_profile(dalex_explainer, variables = "age", type = "accumulated")
plot(pdp_age, geom = "profiles")
```

```{r pdp-all}
pdp_numeric <- model_profile(dalex_explainer, variable_type = "numerical")
plot(pdp_numeric)
plot(pdp_numeric, geom = "points")
plot(model_profile(dalex_explainer, variable_type = "categorical"))
```


### Local explanations

Why a given instance gets its prediction. How could it be changed?

```{r instance-of-interest}
obs_of_interest <- as_tibble(credit_holdout)[5, ]
obs_of_interest

h2o.predict(my_model, as.h2o(obs_of_interest[, features]))
```

### Local Interpretable Model-agnostic Explanation

```{r explain-instance}
model_type.dalex_explainer <- DALEXtra::model_type.dalex_explainer
predict_model.dalex_explainer <- DALEXtra::predict_model.dalex_explainer

lime_explanation <- predict_surrogate(
    explainer = dalex_explainer,
    new_observation = obs_of_interest[, features],  # needs to use a normal df not an H2OFrame!
    type = "lime",
    n_features = 10,  # default: 4
    seed = my_seed  # samples for permutations - still not reproducible :(
)
plot(lime_explanation)
```
Pro:

- model agnostic
- interpretable

Con:

- approximates the black-box model not the data itself
- in high-dimensional data, data points are sparse so defining "local neighborhood" may not be straightforward

### Shapley Additive Explanation

```{r shapley}
# Shapley is most suitable for models with a small or moderate number of explanatory variables

shapley <- predict_parts(
    explainer = dalex_explainer,
    new_observation = obs_of_interest[, features],
    type = "shap",
    B = 20  # number of random orderings to aggregate (default: 25)
)
plot(shapley)
```

Pro:

- model agnostic
- strong formal foundation derived from the cooperative games theory

Con:

- if the model is not additive, SHAP values can mislead
- time-consuming for large models
